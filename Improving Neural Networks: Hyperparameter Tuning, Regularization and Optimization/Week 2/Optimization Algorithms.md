1. Using the notation for mini-batch gradient descent. To what of the following does ğ‘[2]{4}(3)correspond?
    - The activation of the second layer when the input is the third example of the fourth mini-batch.
**In general ğ‘[ğ‘™]{ğ‘¡}(ğ‘˜) denotes the activation of the layer ğ‘™ when the input is the example ğ‘˜ from the mini-batch ğ‘¡.**
2. Suppose you don't face any memory-related problems. Which of the following make more use of vectorization.
    - Batch Gradient Descent
**If no memory problem is faced, batch gradient descent processes all of the training set in one pass, maximizing the use of vectorization.**
3. We usually choose a mini-batch size greater than 1 and less than ğ‘š, because that way we make use of vectorization but not fall into the slower case of batch gradient descent.
    - True
**Precisely by choosing a batch size greater than one we can use vectorization; but we choose a value less than m so we won't end up using batch gradient descent.**
4. While using mini-batch gradient descent with a batch size larger than 1 but less than m, the plot of the cost function ğ½ looks like this: [Fig1] You notice that the value of ğ½ is not always decreasing. Which of the following is the most likely reason for that?
    - In mini-batch gradient descent we calculate J(\hat{y}^{\lbrace t \rbrace}, y^{\lbrace t \rbrace})  thus with each batch we compute over a new set of data.
**Since at each iteration we work with a different set of data or batch the loss function doesn't have to be decreasing at each iteration.**
5. Suppose the temperature in Casablanca over the first two days of March are the following:
March 1st: ğœƒ1=30âˆ˜C
March 2nd: ğœƒ2=15âˆ˜C
Say you use an exponentially weighted average with ğ›½=0.5 to track the temperature: ğ‘£0=0, ğ‘£ğ‘¡=ğ›½ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½)ğœƒğ‘¡. If ğ‘£2 is the value computed after day 2 without bias correction, and ğ‘£2corrected is the value you compute with bias correction. What are these values?
    - v2 = 15, v2 corrected = 20
**v2 =Î²v tâˆ’1 +(1âˆ’Î²)Î¸ t  thus ğ‘£1=15, ğ‘£2=15. Using the bias correction ğ‘£ğ‘¡ / (1 - ğ›½ğ‘¡) we get 15 / (1âˆ’(0.5)^2) =20.**
6. Which of these is NOT a good learning rate decay scheme? Here, ğ‘¡ is the epoch number.
    - alpha = 1.01^t alpha_0
**This is not a good learning rate decay since it is an increasing function of ğ‘¡.**
7. You use an exponentially weighted average on the London temperature dataset. You use the following to track the temperature: ğ‘£ğ‘¡=ğ›½ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½)ğœƒğ‘¡. The red line below was computed using ğ›½=0.9. What would happen to your red curve as you vary ğ›½? (Check the two that apply) [Fig2]
    - Increasing ğ›½ will shift the red line sligthly to the right
**Remember that the red line corresponds to Î²=0.9. In the lecture we had a green line Î²=0.98 that is slightly shifted to the right.**
    - Decreasing ğ›½ will create more oscillation within the red line
**Remember that the red line corresponds to Î²=0.9. In lecture we had a yellow line Î²=0.98 that had a lot of oscillations.**
8. Which of the following are true about gradient descent with momentum?
    - Gradient descent with momentum makes use of moving averages
**Gradient descent with momentum makes use of moving averages, which smooths out the gradient descent process.**
    - Increasing the hyperparameter Î² smooths out the process of gradient descent.
**Gradient descent with momentum makes use of moving averages, which smooths out the gradient descent process.**
    - It generates faster learning by reducing the oscillation of the gradient descent process.
**The use of momentum makes each step of the gradient descent more efficient by reducing oscillations.**
9. Suppose batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function ğ½(ğ‘Š[1],ğ‘[1],...,ğ‘Š[ğ¿],ğ‘[ğ¿]). Which of the following techniques could help find parameter values that attain a small value for ğ½? (Check all that apply)
    - Try using gradient descent with momentum.
**The use of momentum can improve the speed of the training. Although other methods might give better results, such as Adam.**
    - Try better random initialization for the weights.
**As seen in previous lectures this can help the gradient descent process to prevent vanishing gradients.**
    - Normalize the input data
**In some cases, if the scale of the features is very different, normalizing the input data will speed up the training process.**
10. Which of the following are true about Adam?
    - Adam combines the advantages of RMSProp and momentum.
**Precisely Adam combines the features of RMSProp and momentum that is why we use two-parameter ğ›½1 and ğ›½2, besides ğœ–.**
